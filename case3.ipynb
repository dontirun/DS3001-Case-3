{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Data Science in OpenAI Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* [Deep Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/) \n",
    "* [OpenAI](https://gym.openai.com/)\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
    "\n",
    "** NOTE **\n",
    "* In this case study, you will need to use Mac OS or Linux system. In windows computers, you could use virtualbox to create a virtual machine and install an Ubuntu OS in the virtual machine.\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: pick a data science problem that you plan to solve using OpenAI Games\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using the openAI game system and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection/Processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the OpenAI Game System (Gym)\n",
    "\n",
    "**1. Playing Pacman** \n",
    "We first introduce an example of how to use the gym package from OpenAI to design an agent for Pacman Game.\n",
    "In the following cell, we implemented a simple agent, which randomly picks the next action without looking at the screen image (i.e., the *observation*) or the reward.\n",
    "\n",
    "Change the following code for **myAgent** to design a better agent, which takes the observation and reward as the input and picks the best action as the next move.\n",
    "The agent should be able to improve itself after playing more games.\n",
    "\n",
    "***The goals***: Implement an agent using neural networks that can achieve all the following goals:\n",
    "* (a) move the PacMan in all directions.  (5 points)\n",
    "* (b) using neural network to decide what is the best next move. (5 points)\n",
    "* (c) after playing each episode of the game, the agent should be able to improve itself using the experience. (5 points)\n",
    "\n",
    "Action Code:\n",
    "* 1 - UP\n",
    "* 2 - RIGHT\n",
    "* 3 - LEFT\n",
    "* 4 - DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-04 09:44:55,310] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 130.000000. running mean: 130.000000\n",
      "resetting env. episode reward total was 290.000000. running mean: 131.600000\n",
      "resetting env. episode reward total was 170.000000. running mean: 131.984000\n",
      "resetting env. episode reward total was 160.000000. running mean: 132.264160\n",
      "resetting env. episode reward total was 280.000000. running mean: 133.741518\n",
      "resetting env. episode reward total was 160.000000. running mean: 134.004103\n",
      "resetting env. episode reward total was 200.000000. running mean: 134.664062\n",
      "resetting env. episode reward total was 300.000000. running mean: 136.317422\n",
      "resetting env. episode reward total was 260.000000. running mean: 137.554247\n",
      "resetting env. episode reward total was 310.000000. running mean: 139.278705\n",
      "resetting env. episode reward total was 200.000000. running mean: 139.885918\n",
      "resetting env. episode reward total was 190.000000. running mean: 140.387059\n",
      "resetting env. episode reward total was 260.000000. running mean: 141.583188\n",
      "resetting env. episode reward total was 280.000000. running mean: 142.967356\n",
      "resetting env. episode reward total was 140.000000. running mean: 142.937683\n",
      "resetting env. episode reward total was 230.000000. running mean: 143.808306\n",
      "resetting env. episode reward total was 520.000000. running mean: 147.570223\n",
      "resetting env. episode reward total was 160.000000. running mean: 147.694521\n",
      "resetting env. episode reward total was 260.000000. running mean: 148.817575\n",
      "resetting env. episode reward total was 300.000000. running mean: 150.329400\n",
      "resetting env. episode reward total was 320.000000. running mean: 152.026106\n",
      "resetting env. episode reward total was 370.000000. running mean: 154.205844\n",
      "resetting env. episode reward total was 240.000000. running mean: 155.063786\n",
      "resetting env. episode reward total was 280.000000. running mean: 156.313148\n",
      "resetting env. episode reward total was 210.000000. running mean: 156.850017\n",
      "resetting env. episode reward total was 270.000000. running mean: 157.981517\n",
      "resetting env. episode reward total was 220.000000. running mean: 158.601701\n",
      "resetting env. episode reward total was 290.000000. running mean: 159.915684\n",
      "resetting env. episode reward total was 240.000000. running mean: 160.716528\n",
      "resetting env. episode reward total was 230.000000. running mean: 161.409362\n",
      "resetting env. episode reward total was 220.000000. running mean: 161.995269\n",
      "resetting env. episode reward total was 140.000000. running mean: 161.775316\n",
      "resetting env. episode reward total was 190.000000. running mean: 162.057563\n",
      "resetting env. episode reward total was 170.000000. running mean: 162.136987\n",
      "resetting env. episode reward total was 240.000000. running mean: 162.915617\n",
      "resetting env. episode reward total was 210.000000. running mean: 163.386461\n",
      "resetting env. episode reward total was 320.000000. running mean: 164.952596\n",
      "resetting env. episode reward total was 270.000000. running mean: 166.003071\n",
      "resetting env. episode reward total was 230.000000. running mean: 166.643040\n",
      "resetting env. episode reward total was 170.000000. running mean: 166.676609\n",
      "resetting env. episode reward total was 200.000000. running mean: 167.009843\n",
      "resetting env. episode reward total was 210.000000. running mean: 167.439745\n",
      "resetting env. episode reward total was 230.000000. running mean: 168.065347\n",
      "resetting env. episode reward total was 310.000000. running mean: 169.484694\n",
      "resetting env. episode reward total was 260.000000. running mean: 170.389847\n",
      "resetting env. episode reward total was 380.000000. running mean: 172.485949\n",
      "resetting env. episode reward total was 260.000000. running mean: 173.361089\n",
      "resetting env. episode reward total was 220.000000. running mean: 173.827478\n",
      "resetting env. episode reward total was 250.000000. running mean: 174.589203\n",
      "resetting env. episode reward total was 230.000000. running mean: 175.143311\n",
      "resetting env. episode reward total was 400.000000. running mean: 177.391878\n",
      "resetting env. episode reward total was 280.000000. running mean: 178.417959\n",
      "resetting env. episode reward total was 200.000000. running mean: 178.633780\n",
      "resetting env. episode reward total was 190.000000. running mean: 178.747442\n",
      "resetting env. episode reward total was 260.000000. running mean: 179.559968\n",
      "resetting env. episode reward total was 260.000000. running mean: 180.364368\n",
      "resetting env. episode reward total was 190.000000. running mean: 180.460724\n",
      "resetting env. episode reward total was 120.000000. running mean: 179.856117\n",
      "resetting env. episode reward total was 290.000000. running mean: 180.957556\n",
      "resetting env. episode reward total was 380.000000. running mean: 182.947980\n",
      "resetting env. episode reward total was 260.000000. running mean: 183.718501\n",
      "resetting env. episode reward total was 230.000000. running mean: 184.181316\n",
      "resetting env. episode reward total was 280.000000. running mean: 185.139502\n",
      "resetting env. episode reward total was 200.000000. running mean: 185.288107\n",
      "resetting env. episode reward total was 240.000000. running mean: 185.835226\n",
      "resetting env. episode reward total was 170.000000. running mean: 185.676874\n",
      "resetting env. episode reward total was 220.000000. running mean: 186.020105\n",
      "resetting env. episode reward total was 290.000000. running mean: 187.059904\n",
      "resetting env. episode reward total was 270.000000. running mean: 187.889305\n",
      "resetting env. episode reward total was 200.000000. running mean: 188.010412\n",
      "resetting env. episode reward total was 290.000000. running mean: 189.030308\n",
      "resetting env. episode reward total was 220.000000. running mean: 189.340005\n",
      "resetting env. episode reward total was 150.000000. running mean: 188.946605\n",
      "resetting env. episode reward total was 250.000000. running mean: 189.557139\n",
      "resetting env. episode reward total was 210.000000. running mean: 189.761567\n",
      "resetting env. episode reward total was 170.000000. running mean: 189.563952\n",
      "resetting env. episode reward total was 240.000000. running mean: 190.068312\n",
      "resetting env. episode reward total was 380.000000. running mean: 191.967629\n",
      "resetting env. episode reward total was 250.000000. running mean: 192.547953\n",
      "resetting env. episode reward total was 210.000000. running mean: 192.722473\n",
      "resetting env. episode reward total was 980.000000. running mean: 200.595249\n",
      "resetting env. episode reward total was 240.000000. running mean: 200.989296\n",
      "resetting env. episode reward total was 270.000000. running mean: 201.679403\n",
      "resetting env. episode reward total was 180.000000. running mean: 201.462609\n",
      "resetting env. episode reward total was 270.000000. running mean: 202.147983\n",
      "resetting env. episode reward total was 210.000000. running mean: 202.226503\n",
      "resetting env. episode reward total was 280.000000. running mean: 203.004238\n",
      "resetting env. episode reward total was 240.000000. running mean: 203.374196\n",
      "resetting env. episode reward total was 280.000000. running mean: 204.140454\n",
      "resetting env. episode reward total was 200.000000. running mean: 204.099049\n",
      "resetting env. episode reward total was 950.000000. running mean: 211.558059\n",
      "resetting env. episode reward total was 360.000000. running mean: 213.042478\n",
      "resetting env. episode reward total was 260.000000. running mean: 213.512053\n",
      "resetting env. episode reward total was 190.000000. running mean: 213.276933\n",
      "resetting env. episode reward total was 240.000000. running mean: 213.544164\n",
      "resetting env. episode reward total was 290.000000. running mean: 214.308722\n",
      "resetting env. episode reward total was 210.000000. running mean: 214.265635\n",
      "resetting env. episode reward total was 250.000000. running mean: 214.622978\n",
      "resetting env. episode reward total was 190.000000. running mean: 214.376749\n",
      "resetting env. episode reward total was 170.000000. running mean: 213.932981\n",
      "resetting env. episode reward total was 250.000000. running mean: 214.293651\n",
      "resetting env. episode reward total was 250.000000. running mean: 214.650715\n",
      "resetting env. episode reward total was 240.000000. running mean: 214.904208\n",
      "resetting env. episode reward total was 250.000000. running mean: 215.255166\n",
      "resetting env. episode reward total was 260.000000. running mean: 215.702614\n",
      "resetting env. episode reward total was 600.000000. running mean: 219.545588\n",
      "resetting env. episode reward total was 280.000000. running mean: 220.150132\n",
      "resetting env. episode reward total was 310.000000. running mean: 221.048631\n",
      "resetting env. episode reward total was 290.000000. running mean: 221.738144\n",
      "resetting env. episode reward total was 230.000000. running mean: 221.820763\n",
      "resetting env. episode reward total was 270.000000. running mean: 222.302555\n",
      "resetting env. episode reward total was 260.000000. running mean: 222.679530\n",
      "resetting env. episode reward total was 380.000000. running mean: 224.252734\n",
      "resetting env. episode reward total was 240.000000. running mean: 224.410207\n",
      "resetting env. episode reward total was 290.000000. running mean: 225.066105\n",
      "resetting env. episode reward total was 290.000000. running mean: 225.715444\n",
      "resetting env. episode reward total was 230.000000. running mean: 225.758289\n",
      "resetting env. episode reward total was 300.000000. running mean: 226.500707\n",
      "resetting env. episode reward total was 280.000000. running mean: 227.035699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f99267d64db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;31m# step the environment and get new measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m   \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;31m#print \"reward:%f\" % reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arun/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arun/anaconda2/lib/python2.7/site-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arun/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arun/anaconda2/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arun/anaconda2/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#<insert your code here>\n",
    "#This library has a dependency that does not work on Windows. Therefore, I'm going to use the example code that earns 40 points\n",
    "#on its own.\n",
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on PacMan. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False# resume from previous checkpoint?\n",
    "render = False # whether to render the game. You should turn this off to speed up the program.\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  #crop the screen\n",
    "  I = np.reshape(I[0:160], (160,160,3))\n",
    "  I = I[:,:,1]\n",
    "  I[I==111]=0\n",
    "  I[I==28]=1\n",
    "  I[I>1]=2\n",
    "  I=I[::2,::2]\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel() # ravel flaten the matrix into 1-d vector\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "    \n",
    "  action = 2 \n",
    "  y = .5\n",
    "  if(np.random.uniform() < aprob/4):\n",
    "      action = 1\n",
    "      y=.25\n",
    "  elif(np.random.uniform() < aprob/2):\n",
    "      action = 2\n",
    "      y = .5\n",
    "  elif(np.random.uniform() < 3*aprob/4):\n",
    "      action = 3\n",
    "      y=.75\n",
    "  else:\n",
    "      action = 4\n",
    "      y =1\n",
    " \n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  # y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "  #print \"reward:%f\" % reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.iteritems():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for  processing the screen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "env=gym.make('MsPacman-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "# plot the current screen\n",
    "plt.figure(1)\n",
    "plt.imshow(np.reshape(observation, (210,160,3) ))\n",
    "\n",
    "#crop the screen\n",
    "x = np.reshape(observation[0:171], (171,160,3) )\n",
    "plt.figure(2)\n",
    "plt.imshow(x)\n",
    "\n",
    "\n",
    "# remove background\n",
    "x = x[:,:,1]\n",
    "x[x==111]=0\n",
    "x[x==28]=1\n",
    "x[x>1]=2\n",
    "# downsample\n",
    "x=x[::2,::2]\n",
    "plt.figure(3)\n",
    "plt.imshow(x,cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write codes to implement the solution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 3\".\n",
    "        \n",
    "** Note: Each team only needs to submit one submission in Canvas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (10 points) how well did the team describe the problem they are trying to solve using the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand the problem\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection and Processing:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (5 points) Do you think the data collected/processed are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       1: I can barely understand what data they are trying to collect/process\n",
    "       2: I can barely understand why the data is relevant to the problem\n",
    "       3: the data are relevant to the problem, but better data can be collected\n",
    "       4: the data collected are relevant and at a proper scale\n",
    "       5: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the exploration task (15 points):\n",
    "       0: missing answer\n",
    "       5: okay, but with major problems\n",
    "      10: good, but with minor problems\n",
    "      15: perfect\n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? (10 points)\n",
    "       0: not relevant\n",
    "       2: barely relevant to the problem\n",
    "       4: okay solution, but there is an easier solution.\n",
    "       6: good, but can be improved\n",
    "       8: very good, but solution is simple/old\n",
    "       10: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? (10 points)\n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think of the results they found in the data?  (5 points)\n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Overall: \n",
    "    -----------------------------------\n",
    "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "    Total score:\n",
    "    \n",
    "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
    "       1: \n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
    "       1:\n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ---------------------------------\n",
    "    Your Vote: \n",
    "    ---------------------------------\n",
    "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  (5 bonus points)\n",
    "        0: I vote the other team is better than this team\n",
    "        5: I vote this team is better than the other team \n",
    "        \n",
    "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
    "        1: Team 1\n",
    "        2: Team 2\n",
    "        3: Team 3\n",
    "        4: Team 4\n",
    "        5: Team 5\n",
    "        6: Team 6\n",
    "        7: Team 7\n",
    "        8: Team 8\n",
    "        9: Team 9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
